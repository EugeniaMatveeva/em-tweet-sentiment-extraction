{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport math\nimport re\nimport random\nimport string\nfrom tqdm import tqdm\nimport time\nfrom collections import Counter\n\nimport nltk\nimport torch\nimport spacy\nfrom sklearn.preprocessing import binarize\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split, Subset\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup\nimport tokenizers\nfrom tokenizers import BertWordPieceTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain_df.dropna(inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/bert-base-uncased/config.json', 'r') as f:\n    bert_config = f.read()\nbert_config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Loading BERT tokenizer...')\n# PRETRAINED_TOKENIZER = '../input/bert-base-uncased/vocab.txt'\n# tokenizer = tokenizers.BertWordPieceTokenizer(\n#         PRETRAINED_TOKENIZER, \n#         lowercase=True\n#     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16\n    BERT_PATH = \"../input/bert-base-uncased/\"\n    MODEL_PATH = \"pytorch_model.bin\"\n    TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n    PRETRAINED_TOKENIZER = f\"{BERT_PATH}/vocab.txt\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        PRETRAINED_TOKENIZER, \n        lowercase=True\n    )\n    PRETRAINED_MODEL = f\"{BERT_PATH}/{MODEL_PATH}\"\ncfg = Config()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set folds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nfold_size = train_df.shape[0] // n_folds\nprint(f'Train size={train_df.shape[0]}, fold size={fold_size}')\nfree_inds = set(range(train_df.shape[0]))\nfor i in range(n_folds - 1):\n    print(i)\n    fold_inds = np.random.choice(list(free_inds), fold_size, replace=False)\n    train_df.loc[fold_inds, 'fold'] = i\n    free_inds -= set(fold_inds)\nfold_inds = list(free_inds)\ntrain_df.loc[fold_inds, 'fold'] = n_folds - 1\ntrain_df['fold'] = train_df['fold'].astype(int)\ndf = train_df.groupby('fold').count()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def seed_everything(seed_value):\n#     random.seed(seed_value)\n#     np.random.seed(seed_value)\n#     torch.manual_seed(seed_value)\n#     os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n#     if torch.cuda.is_available(): \n#         torch.cuda.manual_seed(seed_value)\n#         torch.cuda.manual_seed_all(seed_value)\n#         torch.backends.cudnn.deterministic = True\n#         torch.backends.cudnn.benchmark = True\n\n# seed = 1234\n# seed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.selected_text = 'selected_text' in df.columns\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def _prepare_data(self, text, selected_text, sentiment):\n        len_selected = len(selected_text)\n#         print(f'text={text}')\n#         print(f'selected_text={selected_text}')\n#         print(f'len_selected={len_selected}')\n       \n        start_pos = None\n        end_pos = None\n        for i, txt in enumerate(text):\n            if text[i: i + len_selected] == selected_text:\n                start_pos = i\n                end_pos = i + len_selected - 1\n                break\n#         print(f'start_pos={start_pos}')\n#         print(f'end_pos={end_pos}')\n        \n        char_targets = [0] * len(text)\n        if start_pos != None and end_pos != None:\n            for ct in range(start_pos, end_pos + 1):\n                char_targets[ct] = 1\n#         print(f'char_targets={char_targets}')\n        \n        text_enc = self.tokenizer.encode(text)\n#         print(f'text_enc={text_enc.tokens}')\n        input_ids_orig = text_enc.ids[1:-1]\n        CLS_ID = text_enc.ids[0]\n        SEP_ID = text_enc.ids[-1]\n#         print(f'input_ids={text_enc.ids}')\n#         print(f'input_ids_orig={input_ids_orig}')\n        text_offsets = text_enc.offsets[1:-1]\n#         print(f'text_offsets={text_offsets}')\n        \n        target_idx = []\n        for j, (offset1, offset2) in enumerate(text_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n#         print(f'target_idx={target_idx}')\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n#         print(f'targets_start={targets_start}')\n#         print(f'targets_end={targets_end}')\n                \n        sentiment_id = self.tokenizer.encode(sentiment).ids[1]\n#         print(f'sentiment_id={sentiment_id}')\n\n        input_ids = [CLS_ID] + [sentiment_id] + [SEP_ID] + input_ids_orig + [SEP_ID]\n        token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        text_offsets = [(0, 0)] * 3 + text_offsets + [(0, 0)]\n        targets_start += 3\n        targets_end += 3\n        \n        padding_length = self.max_len - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            text_offsets = text_offsets + ([(0, 0)] * padding_length)\n\n        return {\n            'ids': input_ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets_start': targets_start,\n            'targets_end': targets_end,\n            'orig_text': text,\n            'orig_selected': selected_text,\n            'sentiment': sentiment,\n            'offsets': text_offsets\n        }\n        \n        return text_enc\n\n    def __getitem__(self, item):\n        try:\n            sel_text = self.df.iloc[item].selected_text if self.selected_text else ''\n            data = self._prepare_data(\n                self.df.iloc[item].text, \n                sel_text, \n                self.df.iloc[item].sentiment\n            )\n        except KeyError:\n            print(f'Bad key: {item}')\n            print(f'Current index: {self.df.index.values}')\n            raise\n                  \n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_text': data[\"orig_text\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }\n    \n    def __len__(self):\n        return len(self.df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = TweetDataset(train_df, cfg.TOKENIZER, cfg.MAX_LEN)\n# itr = iter(train)\n# next(itr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TweetDataset(train_df, cfg.TOKENIZER, cfg.MAX_LEN)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=cfg.TRAIN_BATCH_SIZE,\n    num_workers=1\n)\n\nfor bi, d in enumerate(train_data_loader):\n    ids = d[\"ids\"]\n    token_type_ids = d[\"token_type_ids\"]\n    mask = d[\"mask\"]\n    targets_start = d[\"targets_start\"]\n    targets_end = d[\"targets_end\"]\n    sentiment = d[\"sentiment\"]\n    orig_selected = d[\"orig_selected\"]\n    orig_text = d[\"orig_text\"]\n    targets_start = d[\"targets_start\"]\n    targets_end = d[\"targets_end\"]\n    offsets = d[\"offsets\"]\n    print(orig_selected)    \n    print(sentiment)    \n    print(targets_start)\n    print(targets_end)    \n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using folds instead","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # split train val\n# size = len(train_dataset)\n# print(f'dataset size={size}')\n# val_size = int(size*0.15)\n# print(f'val size={val_size}')\n# train_size = size - val_size\n# print(f'train size={train_size}')\n\n# train, val = random_split(train_dataset, [train_size, val_size])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, bert_config, bert_path, dropout):\n        super(TweetModel, self).__init__(bert_config)\n        self.bert = transformers.BertModel.from_pretrained(bert_path, config=bert_config)\n        self.drop_out = nn.Dropout(dropout)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n#         print(f'len(out)={len(out)}')\n#         print(f'out[0]={out[0].shape}')\n#         print(f'out[-1]={out[-1].shape}')\n#         print(f'out[-2]={out[-2].shape}')\n        out = torch.cat((out[-1], out[-2]), dim=-1)            # out[i].shape = (batch_size, sequence_length, hidden_size)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n#         print(f'logits.shape={logits.shape}')\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n#         print(f'start_logits.shape={start_logits.shape}')\n#         print(f'end_logits.shape={end_logits.shape}')\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n#         print(f'start_logits.shape={start_logits.shape}')\n#         print(f'end_logits.shape={end_logits.shape}')\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    print(f'start_positions={start_positions.shape}')\n    print(f'end_positions={start_logits.shape}')\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if (len(a) + len(b) - len(c)) == 0:\n        return 0\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef evaluate(true, pred):\n    jac = 0\n    for s1, s2 in zip(true, pred):\n        jac += jaccard(s1, s2)\n    jac /= len(true)\n    return jac\n\ndef calculate_jaccard_score(\n    original_text, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output = ''\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_text[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_text.split()) < 2:\n        filtered_output = original_text\n\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler=None, max_grad_norm=1.0):\n    model.to(device)\n    model.train()\n    losses = []\n    jaccards = []\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_text = d[\"orig_text\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for i, text in enumerate(orig_text):\n            selected_text = orig_selected[i]\n            text_sentiment = sentiment[i]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_text=text,\n                target_string=selected_text,\n                sentiment_val=text_sentiment,\n                idx_start=np.argmax(outputs_start[i, :]),\n                idx_end=np.argmax(outputs_end[i, :]),\n                offsets=offsets[i]\n            )\n            jaccard_scores.append(jaccard_score)\n        jaccards.append(np.mean(jaccard_scores))\n        losses.append(loss.item())\n        tk0.set_postfix(loss=np.mean(losses), jaccard=np.mean(jaccards))\n    return losses, jaccards","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_fn(data_loader, model, device):\n    model.to(device)\n    model.eval()\n    losses = []\n    jaccards = []\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_text = d[\"orig_text\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px, text in enumerate(orig_text):\n                selected_text = orig_selected[px]\n                text_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_text=text,\n                    target_string=selected_text,\n                    sentiment_val=text_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.append(np.mean(jaccard_scores))\n            losses.append(loss.item())\n            tk0.set_postfix(loss=np.mean(losses), jaccard=np.mean(jaccards))\n    \n    print(f\"Jaccard = {np.mean(jaccards)}\")\n    return losses, jaccards","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    model_config = transformers.BertConfig.from_pretrained(cfg.BERT_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(bert_config=model_config, bert_path=cfg.BERT_PATH, dropout=0.1)\n    return model\n\ndef get_training_objects():\n    device = torch.device(\"cuda\")\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n            \"weight_decay\": 0.0\n        },\n    ]\n    n_training_steps = math.ceil(len(train_data_loader) / cfg.TRAIN_BATCH_SIZE)*EPOCHS\n    n_warmup_steps = int(n_training_steps*0.2)\n    print(f'n_training_steps={n_training_steps}')\n    print(f'n_warmup_steps={n_warmup_steps}')\n    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-8)\n    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=10)\n    #get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=n_training_steps)\n    return device, optimizer, scheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n\ndef save_model(model, model_file):\n    # Save a model, configuration that you have fine-tuned\n\n    # If we have a distributed model, save only the encapsulated model\n    # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n    model_to_save = model.module if hasattr(model, 'module') else model\n\n    # If we save using the predefined names, we can load using `from_pretrained`\n    output_model_file = model_file + '_' + WEIGHTS_NAME\n    output_config_file = model_file + '_' + CONFIG_NAME\n\n    print(output_model_file)\n    print(output_config_file)\n    torch.save(model_to_save.state_dict(), output_model_file)\n    model_to_save.config.to_json_file(output_config_file)\n\ndef load_model(model_file):\n    # Re-load the saved model and vocabulary\n    output_model_file = model_file + '_' + WEIGHTS_NAME\n    output_config_file = model_file + '_' + CONFIG_NAME\n    config = transformers.BertConfig.from_json_file(output_config_file)\n    model = TweetModel(bert_config=config, bert_path=output_model_file, dropout=0.1)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = np.unique(train_df['fold'].values)\nmodels = []\nfor test_fold in folds:\n    val_part_df = train_df[train_df['fold'] == test_fold]\n    train_part_df = train_df[train_df['fold'] != test_fold]\n    \n    val_dataset = TweetDataset(val_part_df, cfg.TOKENIZER, cfg.MAX_LEN)\n    train_dataset = TweetDataset(train_part_df, cfg.TOKENIZER, cfg.MAX_LEN)\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=cfg.TRAIN_BATCH_SIZE,\n        num_workers=1,\n        shuffle=True\n    )\n    val_data_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            batch_size=cfg.VALID_BATCH_SIZE,\n            num_workers=2,\n            shuffle=False\n        )\n    \n    # model\n    model = get_model()\n    \n    device, optimizer, scheduler = get_training_objects()\n    \n    train_losses, train_jaccards = [], []\n    val_losses, val_jaccards = [], []\n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch}')\n        t_losses, t_jaccards = train_fn(train_data_loader, model, optimizer, device, scheduler)\n        train_losses.append(np.mean(t_losses))\n        train_jaccards.append(np.mean(t_jaccards))\n        v_losses, v_jaccards = eval_fn(val_data_loader, model, device)\n        val_losses.append(np.mean(v_losses))\n        val_jaccards.append(np.mean(v_jaccards))\n        print(f\"Jaccard Score = {val_jaccards}\")\n    save_model(model, f'fold{test_fold}')\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses, train_jaccards = [], []\nval_losses, val_jaccards = [], []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(16,4))\nax.set(xlabel='epoch', ylabel='total loss',\n    title='loss per epoch')\nax.grid()\ntrain_losses = np.array(train_losses)\nax.plot(train_losses, color='b', label='Train loss')\n#ax.plot(val_losses, color='r', label='Validation loss')\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # load models\n# models = []\n# for fold in folds:\n#     models.append(load_model(f'fold{fold}'))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models[0].eval()\n# v_losses, v_jaccards = eval_fn(val_data_loader, models[0], device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # split train val\n# size = len(train_dataset)\n# print(f'dataset size={size}')\n# val_size = int(size*0.15)\n# print(f'val size={val_size}')\n# train_size = size - val_size\n# print(f'train size={train_size}')\n\n# train, val = random_split(train_dataset, [train_size, val_size])\n\n# val_data_loader = torch.utils.data.DataLoader(\n#             val,\n#             batch_size=cfg.VALID_BATCH_SIZE,\n#             num_workers=2,\n#             shuffle=False\n#         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\njaccards = []\ntest_df.loc[:, \"selected_text\"] = test_df.text.values\ntest_dataset = TweetDataset(test_df, cfg.TOKENIZER, cfg.MAX_LEN)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=cfg.VALID_BATCH_SIZE,\n    num_workers=1\n)\n\nwith torch.no_grad():\n    tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_text = d[\"orig_text\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n        \n        # get mean probability\n        starts, ends = [], []\n        for model in models:\n            model.to(device)\n            model.eval()\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            starts.append(outputs_start)\n            ends.append(outputs_end)\n        \n        outputs_start = torch.mean(torch.stack(starts), dim=0)\n        outputs_end = torch.mean(torch.stack(ends), dim=0)\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n        jaccard_scores=[]\n        for px, text in enumerate(orig_text):\n            selected_text = orig_selected[px]\n            text_sentiment = sentiment[px]\n            jaccard_score, output_sentence = calculate_jaccard_score(\n                original_text=text,\n                target_string=selected_text,\n                sentiment_val=text_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            final_output.append(output_sentence)\n            jaccard_scores.append(jaccard_score)\n\n        jaccards.append(np.mean(jaccard_scores))\n        tk0.set_postfix(jaccard=np.mean(jaccards))\nlen(final_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(selected):\n    return \" \".join(set(selected.lower().split()))\n\nsample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.selected_text = sample.selected_text.map(post_process)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}